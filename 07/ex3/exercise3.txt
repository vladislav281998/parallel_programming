1.) Investigate the given code excerpt. Can this code be safely parallelized manually? Can this code be safely parallelized by the compiler?

void copy(double* x, double* y) {
    for(int i = 0; i < 1024; i++) {
        x[i] = y[i];
    }
}

Manual Parallelization:

Yes, this code can be parallelized manually, since there are no dependencies between iterations.

Compiler Parallelization:

Safety: The compiler might not be able to safely parallelize this loop by default.
Reasoning: Modern compilers perform dependency analysis to identify potential hazards in parallelizing loops. 
In this case, the compiler can detect that each iteration accesses elements at the same index in x and y. 
Since these accesses might be writes, the compiler cannot guarantee the order of writes from different threads, 
leading to a potential data race.

2.) Normalize the following loop nest:

for (int i=4; i<=N; i+=9) {
    for (int j=0; j<=N; j+=5) {
        A[i] = 0;
    }
}

Since the second loop does not affect anything, we can either simply ignore it or delete it, since it is superfluous here.

In = N - 4 / 9. 
i = In * 9 - 9 + 4

for(int=1; In<(N-4/9); In++) {
    A[i] = 0; 
    or, also with nested loop 
    for (int j=0; j<=N; j+=5) { 
       A[i] = 0; 
    }
}

3.) Does the following code excerpt hold any dependencies? If not, how would you parallelize it? If yes, what are the distance and direction vectors?

N	M	L	a[i+1][i][i-1]	a[i]b[i]c[i]
1	1	1	    2,1,0	        1,1,1
1	1	2	    2,1,1	        1,1,2
1	1	3	    2,1,2	        1,1,3
1	2	1	    2,2,0	        1,2,1
1	2	2	    2,2,1	        1,2,2
1	2	3	    2,2,2	        1,2,3
1	3	1	    2,3,0	        1,3,1
1	3	2	    2,3,1	        1,3,2
1	3	3	    2,3,2	        1,3,3
2	1	1	    3,1,0	        2,1,1
2	1	2	    3,1,1	        2,1,2
2	1	3	    3,1,2	        2,1,3
2	2	1	    3,2,0	        2,2,1
2	2	2	    3,2,1	        2,2,2
2	2	3	    3,2,2	        2,2,3
2	3	1	    3,3,0	        2,3,1
2	3	2	    3,3,1	        2,3,2
2	3	3	    3,3,2	        2,3,3
3	1	1	    4,1,0	        3,1,1
3	1	2	    4,1,1	        3,1,2
3	1	3	    4,1,2	        3,1,3
3	2	1	    4,2,0	        3,2,1
3	2	2	    4,2,1	        3,2,2
3	2	3	    4,2,2	        3,2,3
3	3	1	    4,3,0	        3,3,1
3	3	2	    4,3,1	        3,3,2
3	3	3	    4,3,2	        3,3,3

Distance vector (1,0,-1)
Direction vector (<, = , >).

maybe we can parallelize it with: 
#pragma omp parallel for collapse(2)
for(int i = 1; i < N; i++) {
  for(int j = 1; j < M; j++) {
    for(int k = 1; k < L; k++) {
      a[i+1][j][k-1] = a[i][j][k] + 5;
    }
  }
}

Data dependencies: The loop updates a[i+1][j][k-1], which depends on the value of a[i][j][k]. 

This dependency restricts how much parallelization is possible.

False sharing: If multiple threads access the same cache line but operate on different elements within it, 
frequent cache invalidation can occur, reducing performance. This is less likely with larger array sizes.
